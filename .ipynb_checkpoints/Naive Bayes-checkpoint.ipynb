{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes ##\n",
    "\n",
    "Before we get into the Naive Bayes algorithm,  we have to understand Bayes Theoram. The Bayes theoram sets out a probabilistic formula for predicting the likelihood(probability) of an event based on the probabilities of certain other events or features.\n",
    "\n",
    "The Bayes theoram, mathematically is as follows:\n",
    "\n",
    "<img src=\"files/bayes.png\">\n",
    "\n",
    "where:\n",
    "\n",
    "P(A) and P(B) are the probabilities of observing A and B independantly.\n",
    "\n",
    "P(A | B) is the probability of observing event A given that B is true.\n",
    "\n",
    "P(B | A) is the probability of observing event B given that A is true.\n",
    "\n",
    "For example: When we are searching for the term 'Sacramento Kings' on a search engine, in order for us to get the results pertaining to the Scramento Kings NBA basketball team, the search engine needs to be able to associate the two words together and not treat them individually, in which case we would get results of images tagged with 'Sacramento' like pictures of city landscapes and images of 'Kings' which could be pictures of crowns or kings from history. This is a classic case of the search engine treating the words as independant entities and hence being 'naive' in its approach. \n",
    "\n",
    "\n",
    "The 'Naive' bit gets added to the Bayes Theoram with the added assumption that each of the features that we are using to make our predictions are independant of each other. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective in this mission to train a classifier such that it will be able to identify if a message is spam or not using the Naive Bayes algorithm. \n",
    "\n",
    "We will break the Naive Bayes formula down using the mission objective.\n",
    "\n",
    "** Prior: P(Spam) **\n",
    "\n",
    "The prior is the probability of a message being spam without considering any other factors.\n",
    "\n",
    "** Posterior: P(Spam/One), P(~Spam/One) **\n",
    "\n",
    "The posterior is the probability of a message being spam, given that our classifier classified it with the value '1'(i.e. classified it as being a spam message), and the the probability of a message not being spam, given that our classifier classified it with the value '1'(i.e. classified it as being a spam message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sensitivity(True Positive Rate): **\n",
    "\n",
    "For our mission, the sensitivity is * P(One/Spam) *, the probability of a message being spam, given that we got a '1' or spam classification assigned to it using our algorithm.\n",
    "\n",
    "\n",
    "** Specificity(True Negative Rate): **\n",
    "\n",
    "Similarly, the specificity is * P(Zero/~Spam) *, the probability of a message being not being spam, given that we got a '0' or not spam classification assigned to it using our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above values, we can calculate our posterior as follows:\n",
    "    \n",
    "`P(Spam/One) = (P(Spam) * P(One/Spam) + P(Spam) * P(One/~Spam)) / P(One)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Understanding our dataset ### \n",
    "\n",
    "\n",
    "We will be using a dataset from the UCI Machine Learning repository which has a very good collection of datasets for experimental research purposes. \n",
    "\n",
    "\n",
    " ** Here's a preview of the data: ** \n",
    "\n",
    "<img src=\"files/dqnb.png\">\n",
    "\n",
    "The columns in the data set are currently not named and as you can see, there are 2 columns. \n",
    "\n",
    "The first column takes two values, 'ham' which signifies that the message is not spam, and 'spam' which signifies that the message is spam. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Instructions:\n",
    "\n",
    "Import the dataset into a pandas dataframe using the read_table method. Also, becasue this is a tab separated dataset\n",
    "we will be using '\\t' as the value for the 'sep' argument which specifies this format. Also, rename the column names\n",
    "by specifying a list ['label, 'sms_message'] to the 'names' argument.\n",
    "\n",
    "Print the first five values of the dataframe with the new column names.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "import pandas as pd\n",
    "# Dataset - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection#\n",
    "df = pd.read_table('/Users/adarshnair/Desktop/DQ_NB/smsspamcollection/SMSSpamCollection',\n",
    "                   sep='\\t', \n",
    "                   header=None, \n",
    "                   names=['label', 'sms_message'])\n",
    "\n",
    "# Output\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning our dataset ###\n",
    "\n",
    "Now that we have a basic understanding of what our dataset looks like, lets convert our labels to binary variables, 0 to represent 'ham' and 1 to represent 'spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Instructions: Convert the values in the 'label' colum to numerical values using map method as follows:\n",
    "{'ham':0, 'spam':1} This maps the 'ham' value to 0 and the 'spam' value to 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training and testing sets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Split the dataset into a training and testing set by using the train_test_split method in sklearn. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Check total number of rows in data\n",
    "print df.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.sms_message, \n",
    "                                                    df.label, \n",
    "                                                    random_state=1)\n",
    "# Training dataset size\n",
    "print X_train.shape\n",
    "# Testing dataset size\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Bag of words ###\n",
    "\n",
    "Spam detection is one of the major applications of machine learning and specifically email spam detection is something most of us take for granted these days. Most major email service providers have pretty efficient spam detection systems built in, such that we rely on their algorithms to take care of this for us.\n",
    "\n",
    "There is one small issue though, most ML algorithms rely on numerical data to be fed into them, and email/sms messages are usually text heavy. To handle this, we will be using sklearns `sklearn.feature_extraction.text.CountVectorizer` method which does 3 things:\n",
    "\n",
    "* It tokenizes the string and gives an integer ID to each token.\n",
    "* It counts the occurrance of each of those tokens.\n",
    "* It normalizes the values of the counts for each token so that extremely common words(words like 'the', 'a', 'an', 'is', 'from', pronouns, etc) don't skew the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, we can covert a collection of documents to a matrix, with each document being a row and each token(word) being the column, and the corresponding values being the frequency of occurrance of each token in that document.\n",
    "\n",
    "We have taken care of the part where the textual data is converted into this matrix, so you can focus on the actual prediction, but if you'd like to take a look at how we did it check the following code snippets, otherwise you can head straight to the problem set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is an option segment showing you how to create a frequency matrix given a corpus of documents. The code is \n",
    "already complete and is meant for you to play aroudn with. Feel free to create your own training sample documents \n",
    "to see how the frequency matrix changes with changes in the documents. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>are</th>\n",
       "      <th>busy</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>later</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   am  are  busy  call  from  hello  home  how  later  me  money  win  you\n",
       "0   0    1     0     0     0      1     0    1      0   0      0    0    1\n",
       "1   0    0     0     0     1      0     1    0      0   0      1    1    0\n",
       "2   1    0     1     1     0      0     0    0      1   1      0    0    0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Let's create some data to play with.\n",
    "train_simple = ['Hello how are you!',\n",
    "                'Win money, from home.',\n",
    "                'I am busy, call me later']\n",
    "\n",
    "'''\n",
    "Get the feature list, that is the word list from our corpus of 3 documents.\n",
    "'''\n",
    "vect = CountVectorizer()\n",
    "vect.fit(train_simple)\n",
    "vect.get_feature_names()\n",
    "\n",
    "'''\n",
    "Create a matrix with the rows being the 3 documents, and the columns being each word. \n",
    "The corresponding (row, column) value is the frequency of occurrance of that word(in the column) in a particular\n",
    "document(in the row).\n",
    "'''\n",
    "train_simple_dtm = vect.transform(train_simple)\n",
    "train_simple_dtm\n",
    "train_simple_dtm.toarray()\n",
    "\n",
    "# Here is the matrix in the form of a dataframe\n",
    "pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "# # transform testing data into a document-term matrix (using existing vocabulary)\n",
    "# test_simple = [\"please don't call me\"]\n",
    "# test_simple_dtm = vect.transform(test_simple)\n",
    "# test_simple_dtm.toarray()\n",
    "# pd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Hidden) Creating Frequency Matrix ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn vocabulary and create document-term matrix in a single step\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix\n",
    "test_dtm = vect.transform(X_test)\n",
    "test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training features:  7456\n",
      "Number of training features:  7456\n"
     ]
    }
   ],
   "source": [
    "train_features = vect.get_feature_names()\n",
    "print 'Number of training features: ', len(train_features)\n",
    "test_features = vect.get_feature_names()\n",
    "print 'Number of training features: ', len(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Naive Bayes Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988513998564\n",
      "[[1203    5]\n",
      " [  11  174]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, y_train)\n",
    "\n",
    "# make predictions on test data using test_dtm\n",
    "preds = nb.predict(test_dtm)\n",
    "preds\n",
    "\n",
    "# compare predictions to true labels\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(y_test, preds)\n",
    "print metrics.confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
